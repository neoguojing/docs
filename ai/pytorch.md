# pytorch

## 张量
- 任何张量的最后一维，都可以理解为「行的长度（每一行的元素个数）
- 向量写法 (3,) 是最合理的写法，因为它忠实反映了数据的维度和结构：
--它不是二维矩阵
--它只有一个轴
--这个轴的长度是 3
### 基于轴的操作
- 沿着 axis 的方向“压扁”它，剩下的方向就保留下来
- 矩阵：axis=0，即沿着行的方向（上下）压缩，axis=1，沿着列的方向压缩
### 张量的算子，对张亮进行的数学运算
- +, -, *, / 对张量元素做逐元素运算
- matmul, t()做矩阵乘法或转置
- sum, mean, max对张量沿指定维度求和、
- reshape： 改变shape，总元素不变
- transpose： 转置，和.T一致
- squeeze：删除张量中长度为1的维度
- unsqueeze：在指定位置增加长度度为1的纬度
- 逻辑/比较运算>, <, ==
- 激活函数ReLU, Sigmoid, Tanh非线性映射，用于神经网络
- 索引/切片x[0,:], gather获取张量的子集或重排
- gather → 类似“根据索引表挑选元素”，特别适合 batch 处理或神经网络输出重排
```
x[0, :]  # 第0行 → tensor([1,2,3])
x[:, 1]  # 第1列 → tensor([2,5,8])
x[1:3, 0:2]  # 第1、2行，第0、1列 → tensor([[4,5],[7,8]])
```
## 自动微分
## 模块化神经网络
## 损失函数
## 优化器
## 数据加载
## 设备管理
## 保存与加载模型
