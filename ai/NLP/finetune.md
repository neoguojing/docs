# 模型微调技术

## LoRA 低秩适应大型语言模型
- 它通过利用低秩矩阵分解来减少微调过程中的计算和内存需求
- 通过将语言模型的权重矩阵进行低秩分解来解决这个问题
- 进行微调时，只需要更新低秩分解后得到的因子矩阵，相比于更新完整的权重矩阵，这需要更少的计算和内存资源
### 奇异值分解（SVD）是一种常用的低秩分解方法，
- 可以将一个矩阵分解为三个部分：左奇异向量矩阵、奇异值对角矩阵和右奇异向量矩阵的转置
- 优点是能够找到最佳的低秩逼近，并且在计算上相对高效
- 奇异值分解对任何矩阵都有效，甚至适用于非方阵
- https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247539565&idx=3&sn=12e3834d5949f3fb6296910ee1a9fa74&chksm=fb3afe66cc4d7770beac01549e482fd83d88513e8bbd7d127115cedc790d9045976ce27ff2b6&scene=27
### 特征分解（Eigenvalue Decomposition）是另一种常见的低秩分解方法
- 它将一个矩阵分解为特征向量矩阵和特征值对角矩阵的乘积形式
- 特征分解在某些情况下可以提供更好的可解释性
- 特征分解的计算复杂度较高
- 特征分解只对方形矩阵有效
