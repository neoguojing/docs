# 从训练到推理

## 模型训练

### 训练好的模型结构

## 转换工具

### 转换好之后的格式


## 模型推理部署

### 模型压缩

#### 模型量化-压缩技术的一种
- 背景： 为了保证较高的精度，大部分的科学运算都是采用浮点型进行计算，常见的是32位浮点型和64位浮点型，即float32和double64。然而推理没有反向传播，网络中存在很多不重要的参数，或者并不需要太细的精度来表示它们；
- 目的：模型量化就是将训练好的深度神经网络的权值，激活值等从高精度转化成低精度的操作过程，例如将32位浮点数转化成8位整型数int8，同时我们期望转换后的模型准确率与转化前相近；
- 结果：
1.减少内存和存储占用；
2.降低功耗；
3.提升计算速度
- 量化对象：
- > 1.weight（权重）：减少模型大小内存和占用空间；
- > 2.activation（激活函数输出）：量化activation不仅可以大大减少内存占用。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升；
- > 3.gradient（梯度）：它主要作用是在分布式计算中减少通信开销，单机训练时也可以减少backward时的开销
- 量化位数：
- > float32/double64 :转换为16位，8位，8位以下（4，2，1）
- 量化分类：
- > 1.离线量化：需要数据，不需要反向传播，数据用于校准 BN，或者统计激活值分布，降低误差；
- > 2.量化感知训练：需要数据，需要反向传播；通过训练和微调使量化模型达到可接受的精度，一般需要完整的训练过程和超参数调整；
均匀量化和非均匀量化
- 量化过程：
- > 在定点与浮点等数据之间建立一种数据映射关系，将信号的连续取值 近似为 有限多个离散值，并使得以较小的精度损失代价获得了较好的收益
- > 公式：Q = round(scale factor * clip（x,α,β））+ zero point
- > zero point： 代表的是原值域中的0在量化后的值
- > round操作：其实就是一种映射关系，决定如何将原来的浮点值按照一定的规律映射到整型值上
- > clip操作：其实就是切片操作，如何来选择这个量化对象的范围
- 量化挑战：
- > 1.受硬件限制；
- > 2.量化误差分析难点；
### 模型推理库
- onnxruntime：onnxruntime支持多种运行后端，包括CPU、GPU、TensorRT、DML
- TensorRT：TensorRT是一个高性能的深度学习推理优化器，可以为深度学习应用提供低延迟、高吞吐率的模型部署；
- OpenVIVO：OpenVIVO是英特尔针对自家硬件平台开发的一套深度学习工具库，包含推理库、模型优化等一些列与深度学习模型部署相关的功能；
- Tengine：Tengine是OPEN AI LAB（开放智能）推理的AI推理框架，致力于解决AIoT应用场景下多厂家多种类的边缘AI芯片与多样的训练框架、算法模型之间的相互兼容适配；
- ncnn：ncnn是一个为手机端极致优化的高性能神经网络前向计算框架；
- mnn：mnn是一个高效、轻量的深度学习框架。它支持深度模型推理与训练，尤其在端侧的推理与训练性能在业界处于领先地位
- tflite：TensorFlowLite是Google在2017年5月推出的轻量级机器学习解决方案，主要针对移动端设备和嵌入式设备
## 运行
