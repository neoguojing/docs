# 从训练到推理

## 模型训练

### 训练好的模型结构

## 转换工具

### 转换好之后的格式


## 模型推理部署

### 模型压缩

#### 模型量化-压缩技术的一种
- 背景： 为了保证较高的精度，大部分的科学运算都是采用浮点型进行计算，常见的是32位浮点型和64位浮点型，即float32和double64。然而推理没有反向传播，网络中存在很多不重要的参数，或者并不需要太细的精度来表示它们；
- 目的：模型量化就是将训练好的深度神经网络的权值，激活值等从高精度转化成低精度的操作过程，例如将32位浮点数转化成8位整型数int8，同时我们期望转换后的模型准确率与转化前相近；
- 结果：
1.减少内存和存储占用；
2.降低功耗；
3.提升计算速度
- 量化对象：
1.weight（权重）：减少模型大小内存和占用空间；
2.activation（激活函数输出）：量化activation不仅可以大大减少内存占用。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升；
3.gradient（梯度）：它主要作用是在分布式计算中减少通信开销，单机训练时也可以减少backward时的开销
- 量化位数：
float32/double64 :转换为16位，8位，8位以下（4，2，1）
- 量化分类：
1.离线量化：需要数据，不需要反向传播，数据用于校准 BN，或者统计激活值分布，降低误差；
2.量化感知训练：需要数据，需要反向传播；通过训练和微调使量化模型达到可接受的精度，一般需要完整的训练过程和超参数调整；
均匀量化和非均匀量化
- 量化过程：
在定点与浮点等数据之间建立一种数据映射关系，将信号的连续取值 近似为 有限多个离散值，并使得以较小的精度损失代价获得了较好的收益
公式：Q = round(scale factor * clip（x,α,β））+ zero point
zero point： 代表的是原值域中的0在量化后的值
round操作：其实就是一种映射关系，决定如何将原来的浮点值按照一定的规律映射到整型值上

### 模型推理库

## 运行
