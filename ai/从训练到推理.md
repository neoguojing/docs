# 从训练到推理

## 模型训练
### 激活函数
- Sigmoid： S行，0-1
- Tanh： s行，-1-1
- ReLU ： > 0,解决梯度消失
- Softmax： 多分类问题的输出层。它将一组实数转换为表示概率分布的向量，使得各个类别的概率之和为 1
- GELU：transformer的激活函数，s行，
### 损失函数-衡量模型预测结果与真实标签之间差异的函数
- 回归问题：均方误差（MSE），平均绝对误差（MAE）
- 分类问题：多分类：交叉熵损失，二分类：对数损失，交叉熵损失
### 优化器：主要作用是根据损失函数的梯度信息，调整模型参数以最小化损失函数，
- SGD：随机梯度下降
- Adam：动量优化器，自适应学习率
- Adagrad：自适应学习率优化器，适合nlp
- Adafactor：自适应学习率优化器，同时对学习率和参数的二阶矩进行估计，对大规模模型和稀疏参数的高效处理
### 训练好的模型结构

## 转换工具

### 转换好之后的格式


## 模型推理部署

### 模型压缩

#### 剪支
在 PyTorch 中实现模型剪枝（Model Pruning）可以按照以下步骤进行：
确定剪枝策略：确定剪枝的策略和准则，以决定哪些参数应该被剪枝。
常见的剪枝策略包括：

- 重要度剪枝（Magnitude-based Pruning）：根据参数的绝对值大小来确定重要性，剪枝低于某个阈值的参数。
- 敏感度剪枝（Sensitivity-based Pruning）：根据参数对损失函数的敏感度来确定重要性，剪枝敏感度低于某个阈值的参数。
- 结构化剪枝（Structured Pruning）：对于卷积层，可以剪枝整个通道（channel）或卷积核（filter）；对于全连接层，可以剪枝整个神经元（neuron）。
- 执行剪枝操作：根据剪枝策略，对模型进行实际的剪枝操作。这通常包括以下步骤：

微调剪枝后的模型：在剪枝后，模型的性能可能会降低。为了恢复或提高性能，需要对剪枝后的模型进行微调（Fine-tuning）。这可以通过在剪枝后的模型上继续进行训练，使用原始的训练集和一小部分验证集。微调的目标是使剪枝后的模型重新适应数据，以提高性能。
```
# 执行剪枝操作
def prune_model(model, pruning_method, pruning_params):
    # 选择剪枝方法和参数
    method = getattr(prune, pruning_method)
    method(model, **pruning_params)
    
    # 移除剪枝相关的参数
    model.apply(prune.remove)
# 执行剪枝操作
pruning_method = 'l1_unstructured'  # 选择剪枝方法，例如 'l1_unstructured' 表示基于 L1 范数进行剪枝
pruning_params = {'name': 'weight', 'amount': 0.5}  # 剪枝参数，例如剪枝权重的一半
prune_model(model, pruning_method, pruning_params)
```
PyTorch 中，有几种常用的剪枝方法可供选择。以下是其中几种常见的剪枝方法：

- L1 范数剪枝（L1-norm Pruning）：基于参数的 L1 范数来确定参数的重要性。通过将参数的绝对值按照阈值进行剪枝，将小于阈值的参数置零。这种剪枝方法可以促使模型稀疏化，即减少非零参数的数量。

- L2 范数剪枝（L2-norm Pruning）：基于参数的 L2 范数来确定参数的重要性。通过将参数的平方和按照阈值进行剪枝，将小于阈值的参数置零。与 L1 范数剪枝不同，L2 范数剪枝不会产生稀疏性，但可以减小参数的绝对值。

- 敏感度剪枝（Sensitivity-based Pruning）：根据参数对损失函数的敏感度来确定参数的重要性。通过计算参数对损失函数的梯度或 Hessian 矩阵等敏感度指标，选择敏感度低于阈值的参数进行剪枝。

- 结构化剪枝（Structured Pruning）：不仅剪枝单个参数，还剪枝整个通道（channel）或卷积核（filter）。这种剪枝方法可以减少模型的计算量和存储需求。

- 全局剪枝（Global Pruning）：在整个模型中统一应用剪枝操作，将参数剪枝比例或阈值应用于所有层的参数。
#### 蒸馏

#### 模型量化-压缩技术的一种
- 背景： 为了保证较高的精度，大部分的科学运算都是采用浮点型进行计算，常见的是32位浮点型和64位浮点型，即float32和double64。然而推理没有反向传播，网络中存在很多不重要的参数，或者并不需要太细的精度来表示它们；
- 目的：模型量化就是将训练好的深度神经网络的权值，激活值等从高精度转化成低精度的操作过程，例如将32位浮点数转化成8位整型数int8，同时我们期望转换后的模型准确率与转化前相近；
- 结果：
1.减少内存和存储占用；
2.降低功耗；
3.提升计算速度
- 量化对象：
- > 1.weight（权重）：减少模型大小内存和占用空间；
- > 2.activation（激活函数输出）：量化activation不仅可以大大减少内存占用。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升；
- > 3.gradient（梯度）：它主要作用是在分布式计算中减少通信开销，单机训练时也可以减少backward时的开销
- 量化位数：
- > float32/double64 :转换为16位，8位，8位以下（4，2，1）
- 量化分类：
- > 1.离线量化：需要数据，不需要反向传播，数据用于校准 BN，或者统计激活值分布，降低误差；
- > 2.量化感知训练：需要数据，需要反向传播；通过训练和微调使量化模型达到可接受的精度，一般需要完整的训练过程和超参数调整；
均匀量化和非均匀量化
- 量化过程：
- > 在定点与浮点等数据之间建立一种数据映射关系，将信号的连续取值 近似为 有限多个离散值，并使得以较小的精度损失代价获得了较好的收益
- > 公式：Q = round(scale factor * clip（x,α,β））+ zero point
- > zero point： 代表的是原值域中的0在量化后的值
- > round操作：其实就是一种映射关系，决定如何将原来的浮点值按照一定的规律映射到整型值上
- > clip操作：其实就是切片操作，如何来选择这个量化对象的范围
- 量化挑战：
- > 1.受硬件限制；
- > 2.量化误差分析难点；
### 模型推理库
- onnxruntime：onnxruntime支持多种运行后端，包括CPU、GPU、TensorRT、DML
- TensorRT：TensorRT是一个高性能的深度学习推理优化器，可以为深度学习应用提供低延迟、高吞吐率的模型部署；
- OpenVIVO：OpenVIVO是英特尔针对自家硬件平台开发的一套深度学习工具库，包含推理库、模型优化等一些列与深度学习模型部署相关的功能；
- Tengine：Tengine是OPEN AI LAB（开放智能）推理的AI推理框架，致力于解决AIoT应用场景下多厂家多种类的边缘AI芯片与多样的训练框架、算法模型之间的相互兼容适配；
- ncnn：ncnn是一个为手机端极致优化的高性能神经网络前向计算框架；
- mnn：mnn是一个高效、轻量的深度学习框架。它支持深度模型推理与训练，尤其在端侧的推理与训练性能在业界处于领先地位
- tflite：TensorFlowLite是Google在2017年5月推出的轻量级机器学习解决方案，主要针对移动端设备和嵌入式设备
## 运行
